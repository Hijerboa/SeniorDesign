{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e5c6da",
   "metadata": {},
   "source": [
    "## Install and Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd846575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "     |████████████████████████████████| 78 kB 10.1 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.1.5)\n",
      "Requirement already satisfied: sklearn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.2-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "     |█████████████████████▉          | 313.0 MB 139.7 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |████████████████████████████████| 458.3 MB 12 kB/s               \n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "     |████████████████████████████████| 3.5 MB 95.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 3)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 3)) (2021.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 4)) (0.24.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 5)) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 5)) (3.17.2)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "     |████████████████████████████████| 462 kB 145.0 MB/s            \n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.44.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "     |████████████████████████████████| 4.3 MB 85.8 MB/s            \n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 5)) (3.1.0)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 2.6 MB/s             \n",
      "\u001b[?25hCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 5)) (1.12.1)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 757 kB/s              \n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.7,>=2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 108.3 MB/s            \n",
      "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 143.5 MB/s            \n",
      "\u001b[?25hCollecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "     |████████████████████████████████| 5.6 MB 71.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (3.0.12)\n",
      "Collecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Downloading tokenizers-0.11.5-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "     |████████████████████████████████| 6.8 MB 66.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (4.5.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 1.4 MB/s              \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (0.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (4.61.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (2021.4.4)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "     |████████████████████████████████| 895 kB 69.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 6)) (2.4.7)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 106.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->-r requirements.txt (line 5)) (1.30.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->-r requirements.txt (line 5)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->-r requirements.txt (line 5)) (2.0.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 72.6 MB/s            \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 15.9 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 6)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 6)) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers->-r requirements.txt (line 6)) (2021.5.30)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers->-r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers->-r requirements.txt (line 6)) (8.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->-r requirements.txt (line 5)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->-r requirements.txt (line 5)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->-r requirements.txt (line 5)) (4.2.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->-r requirements.txt (line 5)) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 135.6 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: typing, clang, termcolor\n",
      "  Building wheel for typing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26432 sha256=d2312d552dc9a1ec14efb63dc83c6c77503c783ea3b12c0a7b33205dd32f9138\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5f/63/c2/b85489bbea28cb5d36cfe197244f898428004fa3caa7a23116\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30878 sha256=aae06e6c3060adf9bd3b9dcef78e751aa57e4f6f41d499e9a28b03d7fd980582\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4850 sha256=c8482ff57093f8776f5ae183c990dcc10572ffd3d48f6849f3bfce955e0f1f25\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built typing clang termcolor\n",
      "Installing collected packages: typing-extensions, six, oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, tokenizers, termcolor, tensorflow-estimator, tensorboard, sacremoses, opt-einsum, keras-preprocessing, keras, huggingface-hub, gast, flatbuffers, clang, astunparse, typing, transformers, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "spacy 3.0.6 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.8.2 which is incompatible.\n",
      "aiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.23.46 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 grpcio-1.44.0 huggingface-hub-0.4.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 sacremoses-0.0.47 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.6.2 tensorflow-estimator-2.6.0 termcolor-1.1.0 tokenizers-0.11.5 transformers-4.16.2 typing-3.7.4.3 typing-extensions-3.7.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "import os, time\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060a8ae",
   "metadata": {},
   "source": [
    "## Declare Globals and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd686a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "INPUT_FEATURES_LOGGING = True\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "# Tokenizer - preprocessing, prepares inputs for the model\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, model_file: str, csv_file: str, csv_size: int, dataset_columns: List[str], test_labels: List[int]):\n",
    "        self.model_file = model_file\n",
    "        self.csv_file = csv_file\n",
    "        self.csv_size = csv_size\n",
    "        self.dataset_columns = dataset_columns\n",
    "        self.test_labels = test_labels\n",
    "\n",
    "preprocessed_1600000_dataset_config = Config(\n",
    "    \"TRAINED_MODEL\",\n",
    "    \"../Data/training.1600000.processed.noemoticon.csv\",\n",
    "    1600000,\n",
    "    [\"polarity\", \"ids\", \"date\", \"query\", \"user\", \"text\"],\n",
    "    [0, 2, 4]\n",
    ")\n",
    "\n",
    "vader_maxval_dataset_config = Config(\n",
    "    \"TRAINED_MODEL_VADER_MAXVAL\",\n",
    "    \"../Data/classified_vader_scored_tweets_max_val.csv\",\n",
    "    30009,\n",
    "    [\"polarity\", \"text\"],\n",
    "    [-1, 0, 1])\n",
    "\n",
    "\n",
    "vader_compval_dataset_config = Config(\n",
    "    \"TRAINED_MODEL_VADER_COMPVAL\",\n",
    "    \"../Data/classified_vader_scored_tweets_compound_val.csv\",\n",
    "    30009,\n",
    "    [\"polarity\", \"text\"],\n",
    "    [-1, 0, 1])\n",
    "\n",
    "manual_dataset_config = Config(\n",
    "    \"TRAINED_MODEL_MANUAL\",\n",
    "    \"../Data/testdata.manual.2009.06.14.csv\",\n",
    "    500,\n",
    "    [\"polarity\", \"ids\", \"date\", \"query\", \"user\", \"text\"],\n",
    "    [0, 2, 4])\n",
    "\n",
    "ian_dataset_config = Config(\n",
    "    \"TRAINED_MODEL_IAN\",\n",
    "    \"../Data/ian.csv\",\n",
    "    250,\n",
    "    [\"polarity\", \"text\"],\n",
    "    [-1, 0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f936e0ce",
   "metadata": {},
   "source": [
    "## Data Handling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2192e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_as_df(dataset_path: str, columns: List[str]):\n",
    "    print(\"Open file:\", dataset_path)\n",
    "    return pd.read_csv(dataset_path, encoding =DATASET_ENCODING , names=columns)\n",
    "\n",
    "\n",
    "def get_tf_dataset(input_examples):\n",
    "    \"\"\"\n",
    "    Converts a dataframe of Input Examples\n",
    "    :return: dataset (tensorflow object) of tensors\n",
    "    \"\"\"\n",
    "    # What we are inputing into the vector\n",
    "    features = []\n",
    "    i = 0\n",
    "    for e in list(input_examples):\n",
    "        i += 1\n",
    "        if i % 10000 == 0 and INPUT_FEATURES_LOGGING:\n",
    "            print(f'{i} | {e}')\n",
    "        input_dict = TOKENIZER.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=280,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"], input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label))\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    tf_dataset = tf.data.Dataset.from_generator(\n",
    "        generator=gen,\n",
    "        output_types=({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        output_shapes=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fb57a",
   "metadata": {},
   "source": [
    "## Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21bc224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_file: str):\n",
    "    location = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "    new_model = TFBertForSequenceClassification.from_pretrained(os.path.join(location, model_file), num_labels=3)\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd90f6",
   "metadata": {},
   "source": [
    "## Model Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d1ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, tweet: str, labels: List[int]):\n",
    "    tf_batch = TOKENIZER(tweet, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = model(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    tf_predictions = np.asarray(tf_predictions).tolist()\n",
    "    if abs(tf_predictions[0][0] - tf_predictions[0][1]) < 0.00:\n",
    "        return labels[1]\n",
    "    elif tf_predictions[0][0] > tf_predictions[0][1]:\n",
    "        return labels[0]\n",
    "    else:\n",
    "        return labels[2]\n",
    "        \n",
    "\n",
    "def test_model(model, config: Config):\n",
    "    \"\"\"\n",
    "    model: object returned from load_model()\n",
    "    config: Config for the file you are testing\n",
    "    \"\"\"\n",
    "    df = get_csv_as_df(config.csv_file, config.dataset_columns)\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if index % 100 == 0:\n",
    "            print(f\"running {index}\")\n",
    "        if row['polarity'] == make_prediction(model, row['text'], config.test_labels):\n",
    "            i += 1\n",
    "    print(f\"TESTING TOOK {(time.time() - start_time) / config.csv_size} per tweet\")\n",
    "    print(f\"Accuracy: {i/config.csv_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4561587",
   "metadata": {},
   "source": [
    "## Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752db2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRAIN(input_config: Config, output_config: Config):\n",
    "    \"\"\"\n",
    "    input_config: pass None if starting from base model, otherwise pass config for model you want to fine tune\n",
    "    output_config: Pass config for resulting model after training completes\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\nLOADING DATA\\n')\n",
    "    df = get_csv_as_df(output_config.csv_file, output_config.dataset_columns)\n",
    "    # remove unnecessary columns / cleanup text / map to our sentiment scores\n",
    "    data_df = pd.DataFrame({\n",
    "        'polarity': df['polarity'].apply(lambda x: x),\n",
    "        'text': df['text'].replace(r'\\n', '', regex=True)\n",
    "    })\n",
    "\n",
    "    # Converting data into proper input format for training the model\n",
    "    # InputExamples are fed to the tokenizer. \n",
    "    input_examples = data_df.apply(lambda x: InputExample(None, text_a=x['text'], text_b=None, label=x['polarity']), axis=1)\n",
    "    train_input_examples, validation_input_examples = train_test_split(input_examples, test_size=0.2)\n",
    "\n",
    "    # Converting data to TensorFlow dataset objects\n",
    "    ### \"get_tf_dataset\" IS THE ONLY PART I HAVENT CHECKED. JUST HOPING IT WORKS ###\n",
    "    tf_train_dataset = get_tf_dataset(list(train_input_examples))\n",
    "    tf_validation_dataset = get_tf_dataset(list(validation_input_examples))\n",
    "\n",
    "    # Shuffle Data\n",
    "    train_data = tf_train_dataset.shuffle(int(output_config.csv_size/10*0.8)).batch(BATCH_SIZE).repeat(2)\n",
    "    validation_data = tf_validation_dataset.shuffle(int(output_config.csv_size/10*0.2)).batch(BATCH_SIZE).repeat(2)\n",
    "\n",
    "    print('\\nLOADING MODEL\\n')\n",
    "    if input_config == None:\n",
    "        print('NO INPUT CONFIG PROVIDED, LOADING BASE MODEL')\n",
    "        model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "    else:\n",
    "        model = load_model(input_config.model_file)\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "    )\n",
    "    \n",
    "    print('\\nTRAINING MODEL\\n')\n",
    "    model.fit(train_data, epochs=EPOCHS, validation_data=validation_data)\n",
    "\n",
    "    print('\\nSAVING MODEL\\n')\n",
    "    location = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "    path = os.path.join(location, output_config.model_file)\n",
    "    model.save_pretrained(path, save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20967d",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06402e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOADING DATA\n",
      "\n",
      "Open file: ../Data/classified_vader_scored_tweets_compound_val.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 | InputExample(guid=None, text_a='COVID-19 Relief Bill - DocumentCloud can anyone tell me how many times â\\x80\\x9cforeign assistanceâ\\x80\\x9d is stated in this 5593 page document?  https://t.co/uVTHY0vtom', text_b=None, label=1.0)\n",
      "20000 | InputExample(guid=None, text_a='A special prosecutor is a step forward but we still need an independent commission to provide the American people with the whole truth. https://t.co/VcQAK1kaaI', text_b=None, label=2.0)\n",
      "\n",
      "LOADING MODEL\n",
      "\n",
      "NO INPUT CONFIG PROVIDED, LOADING BASE MODEL\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cb2b96bb154fec96ff11cd80c34b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING MODEL\n",
      "\n",
      "   3193/Unknown - 7125s 2s/step - loss: 0.4935 - accuracy: 0.7825"
     ]
    }
   ],
   "source": [
    "TRAIN(None, vader_compval_dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665ec08",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5b3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
