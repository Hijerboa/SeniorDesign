{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e638045e",
   "metadata": {},
   "source": [
    "## Install and Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b78c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "import os, time\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf063d",
   "metadata": {},
   "source": [
    "## Declare Globals and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "INPUT_FEATURES_LOGGING = True\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "# Tokenizer - preprocessing, prepares inputs for the model\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, model_file: str, csv_file: str, csv_size: int, dataset_columns: List[str], test_labels: List[int], polarity_lambda):\n",
    "        self.model_file = model_file\n",
    "        self.csv_file = csv_file\n",
    "        self.csv_size = csv_size\n",
    "        self.dataset_columns = dataset_columns\n",
    "        self.test_labels = test_labels\n",
    "        self.polarity_lambda = polarity_lambda\n",
    "\n",
    "preprocessed_1600000_dataset_config = Config(\n",
    "    \"TRAINED_MODEL\",\n",
    "    \"../Data/training.1600000.processed.noemoticon.csv\",\n",
    "    1600000,\n",
    "    [\"polarity\", \"ids\", \"date\", \"query\", \"user\", \"text\"],\n",
    "    [0, 2, 4],\n",
    "    lambda x: int(x/2)\n",
    ")\n",
    "\n",
    "vader_maxval_dataset_config = Config(\n",
    "    \"TRAINED_MODEL_VADER_MAXVAL\",\n",
    "    \"../Data/classified_vader_scored_tweets_max_val.csv\",\n",
    "    30000,\n",
    "    [\"polarity\", \"text\"],\n",
    "    [-1, 0, 1],\n",
    "    lambda x: x+1\n",
    ")\n",
    "\n",
    "\n",
    "vader_compval_dataset_config = Config(\n",
    "    \"TRAINED_MODEL_VADER_COMPVAL\",\n",
    "    \"../Data/classified_vader_scored_tweets_compound_val.csv\",\n",
    "    30000,\n",
    "    [\"polarity\", \"text\"],\n",
    "    [-1, 0, 1],\n",
    "    lambda x: x+1\n",
    ")\n",
    "\n",
    "manual_dataset_config = Config(\n",
    "    \"TRAINED_MODEL_MANUAL\",\n",
    "    \"../Data/testdata.manual.2009.06.14.csv\",\n",
    "    500,\n",
    "    [\"polarity\", \"ids\", \"date\", \"query\", \"user\", \"text\"],\n",
    "    [0, 2, 4],\n",
    "    lambda x: int(x/2)\n",
    ")\n",
    "\n",
    "ian_dataset_config = Config(\n",
    "    \"TRAINED_MODEL_IAN\",\n",
    "    \"../Data/ian.csv\",\n",
    "    250,\n",
    "    [\"polarity\", \"text\"],\n",
    "    [-1, 0, 1],\n",
    "    lambda x: x+1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e4789",
   "metadata": {},
   "source": [
    "## Data Handling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_as_df(dataset_path: str, columns: List[str]):\n",
    "    print(\"Open file:\", dataset_path)\n",
    "    return pd.read_csv(dataset_path, encoding =DATASET_ENCODING , names=columns)\n",
    "\n",
    "\n",
    "def get_tf_dataset(input_examples):\n",
    "    \"\"\"\n",
    "    Converts a dataframe of Input Examples\n",
    "    :return: dataset (tensorflow object) of tensors\n",
    "    \"\"\"\n",
    "    # What we are inputing into the vector\n",
    "    features = []\n",
    "    i = 0\n",
    "    for e in list(input_examples):\n",
    "        i += 1\n",
    "        if i % 10000 == 0 and INPUT_FEATURES_LOGGING:\n",
    "            print(f'{i} | {e}')\n",
    "        input_dict = TOKENIZER.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=280,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"], input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "        features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label))\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    tf_dataset = tf.data.Dataset.from_generator(\n",
    "        generator=gen,\n",
    "        output_types=({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        output_shapes=(\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd268ed",
   "metadata": {},
   "source": [
    "## Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_file: str):\n",
    "    location = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "    new_model = TFBertForSequenceClassification.from_pretrained(os.path.join(location, model_file), num_labels=3)\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63633bb",
   "metadata": {},
   "source": [
    "## Model Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa62fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, tweet: str, labels: List[int]):\n",
    "    tf_batch = TOKENIZER(tweet, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = model(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    prediction = tf.argmax(tf_predictions, axis=1).numpy()\n",
    "    return labels[prediction[0]]\n",
    "        \n",
    "\n",
    "def test_model(model, config: Config):\n",
    "    \"\"\"\n",
    "    model: object returned from load_model()\n",
    "    config: Config for the file you are testing\n",
    "    \"\"\"\n",
    "    df = get_csv_as_df(config.csv_file, config.dataset_columns)\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if index % 100 == 0:\n",
    "            print(f\"running {index}\")\n",
    "        if row['polarity'] == make_prediction(model, row['text'], config.test_labels):\n",
    "            i += 1\n",
    "    print(f\"TESTING TOOK {(time.time() - start_time) / config.csv_size} per tweet\")\n",
    "    print(f\"Accuracy: {i/config.csv_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae3281",
   "metadata": {},
   "source": [
    "## Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRAIN(input_config: Config, output_config: Config):\n",
    "    \"\"\"\n",
    "    input_config: pass None if starting from base model, otherwise pass config for model you want to fine tune\n",
    "    output_config: Pass config for resulting model after training completes\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\nLOADING DATA\\n')\n",
    "    df = get_csv_as_df(output_config.csv_file, output_config.dataset_columns)\n",
    "    # remove unnecessary columns / cleanup text / map to our sentiment scores\n",
    "    data_df = pd.DataFrame({\n",
    "        'polarity': df['polarity'].apply(output_config.polarity_lambda),\n",
    "        'text': df['text'].replace(r'\\n', '', regex=True)\n",
    "    })\n",
    "\n",
    "    # Converting data into proper input format for training the model\n",
    "    # InputExamples are fed to the tokenizer. \n",
    "    input_examples = data_df.apply(lambda x: InputExample(None, text_a=x['text'], text_b=None, label=x['polarity']), axis=1)\n",
    "    train_input_examples, validation_input_examples = train_test_split(input_examples, test_size=0.2)\n",
    "\n",
    "    # Converting data to TensorFlow dataset objects\n",
    "    ### \"get_tf_dataset\" IS THE ONLY PART I HAVENT CHECKED. JUST HOPING IT WORKS ###\n",
    "    tf_train_dataset = get_tf_dataset(list(train_input_examples))\n",
    "    tf_validation_dataset = get_tf_dataset(list(validation_input_examples))\n",
    "\n",
    "    # Shuffle Data\n",
    "    train_data = tf_train_dataset.shuffle(int(output_config.csv_size/10*0.8)).batch(BATCH_SIZE).repeat(2)\n",
    "    validation_data = tf_validation_dataset.shuffle(int(output_config.csv_size/10*0.2)).batch(BATCH_SIZE).repeat(2)\n",
    "\n",
    "    print('\\nLOADING MODEL\\n')\n",
    "    if input_config == None:\n",
    "        print('NO INPUT CONFIG PROVIDED, LOADING BASE MODEL')\n",
    "        model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "    else:\n",
    "        model = load_model(input_config.model_file)\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "    )\n",
    "    \n",
    "    print('\\nTRAINING MODEL\\n')\n",
    "    model.fit(train_data, epochs=EPOCHS, validation_data=validation_data)\n",
    "\n",
    "    print('\\nSAVING MODEL\\n')\n",
    "    location = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "    path = os.path.join(location, output_config.model_file)\n",
    "    model.save_pretrained(path, save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b3595",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN(None, vader_compval_dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173db4a",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b93e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
